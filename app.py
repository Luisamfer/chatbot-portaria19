import streamlit as st
import os
import requests

from langchain_community.document_loaders import BSHTMLLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from langchain import hub
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="Chatbot da Portaria n¬∫ 19/2025", layout="wide")

# Logo do CAF (coloque a imagem em uma pasta chamada 'assets')
st.image("assets/logo_caf.png", width=400)

# T√≠tulo e introdu√ß√£o
st.title("Chatbot da Portaria n¬∫ 19/2025 - MDA")
st.markdown("""
Este assistente virtual foi desenvolvido para responder d√∫vidas com base no conte√∫do oficial da **Portaria n¬∫ 19, de 21 de mar√ßo de 2025**, publicada pelo Minist√©rio do Desenvolvimento Agr√°rio e Agricultura Familiar (MDA).

Voc√™ pode perguntar, por exemplo:
- *Quais documentos s√£o necess√°rios para o CAF?*
- *Onde posso emitir o CAF?*
- *O CAF √© gratuito?*

Digite sua pergunta abaixo e receba uma resposta baseada diretamente no texto da portaria.
""")

# Inicializa o hist√≥rico na sess√£o
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Chave da OpenAI
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY") or st.text_input("üîë Insira sua chave da OpenAI:", type="password")

# Baixar portaria se necess√°rio
html_path = "portaria19.html"
if not os.path.exists(html_path):
    url = "https://www.in.gov.br/web/dou/-/portaria-n-19-de-21-de-marco-de-2025-619527337"
    response = requests.get(url)
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(response.text)

# Carregamento de documentos e vetores
@st.cache_data
def carregar_documentos():
    loader = BSHTMLLoader(html_path)
    dados = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    return splitter.split_documents(dados)

@st.cache_resource
def carregar_vectorstore():
    documentos = carregar_documentos()
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    db = FAISS.from_documents(documentos, embeddings)
    return db

if OPENAI_API_KEY:
    vector_db = carregar_vectorstore()

    def format_docs(documentos):
        return "\n\n".join(doc.page_content for doc in documentos)

    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model="gpt-4o-mini")
    prompt_template = hub.pull("rlm/rag-prompt")

    rag = (
        {
            "question": RunnablePassthrough(),
            "context": vector_db.as_retriever(k=5) | format_docs,
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )

    # Campo de entrada
    pergunta = st.text_input("‚úçÔ∏è Fa√ßa sua pergunta sobre a Portaria:")

    if pergunta:
        with st.spinner("Gerando resposta..."):
            resposta = rag.invoke(pergunta)
            st.session_state.chat_history.append(("Voc√™", pergunta))
            st.session_state.chat_history.append(("Assistente", resposta))

    # Exibir hist√≥rico de perguntas e respostas
    for autor, mensagem in st.session_state.chat_history:
        if autor == "Voc√™":
            st.markdown(f"**üßë‚Äçüíº {autor}:** {mensagem}")
        else:
            st.markdown(f"**ü§ñ {autor}:** {mensagem}")
else:
    st.warning("Por favor, insira sua chave da OpenAI.")
